{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fdk6YyUjvbHW",
        "WCrHf9PfL4rE"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivanrnarvaez/text_mining/blob/main/2_TextRepresentation_tecnicasBoW_master_alumnos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A48-kUHmOmQZ"
      },
      "source": [
        "\n",
        "# Representación numérica de textos\n",
        "En la mayoría de técnicas empleadas en Text Mining y Procesamiento de Lenguaje Natural, es fundamental transformar los textos en representaciones numéricas que puedan ser procesadas por modelos estadísticos o de aprendizaje automático. Aunque existen distintas formas de lograr esta transformación, todas ellas se basan en la misma idea: proyectar el contenido textual en un espacio vectorial, es decir, convertir palabras o documentos en vectores de números.\n",
        "\n",
        "\n",
        "Por una parte, nos encontramos **estrategias léxicas** para representar documentos completos en el espacio vectorial. En estas técnicas, últimamente en desuso en producción pero muy útiles para llevar a cabo análisis preliminares, se construyen matrices que reflejan la frecuencia o relevancia de los términos en cada documento. Este tipo de representación, basada en la **presencia y frecuencia de palabras de un vocabulario en un documento**, resulta especialmente útil en tareas como la clasificación de textos, el topic modeling o la búsqueda de información en grandes volúmenes de datos.\n",
        "\n",
        "Por otro lado, existen **estrategias semánticas** que permiten representar los textos a un nivel más granular, es decir, palabra por palabra. Estas representaciones, conocidas como **embeddings**, asignan a cada token un vector numérico que encapsula parte de su significado semántico. A diferencia de los modelos más simples, los embeddings permiten capturar similitudes y relaciones semánticas entre palabras, lo que los hace muy eficaces para tareas más complejas como la extracción de entidades, el análisis semántico o el entendimiento contextual del lenguaje. Además, a **partir de estos vectores individuales es posible construir representaciones vectoriales de documentos completos**\n",
        "\n",
        "En este notebook se trabajará en el primer caso, la representación vectorial a nivel de documento utliizando técnicas léxicas. En el siguiente notebook exploraremos el mundo de los embeddings y su semántica.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Nociones sobre representación numérica de documentos\n",
        "\n",
        "En este notebook vamos a aprender a representar numéricamente documentos a través de los métodos de representación Bag-of-Words (BoW) y TF-IDF.\n",
        "\n",
        "\n",
        "Para este ejercicio trabajaremos con un corpus muy sencillo, para entender la nocion del funcionamiento de estas técnicas de representación:"
      ],
      "metadata": {
        "id": "OW1pFXcKTOMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"yo quiero agua\",\n",
        "          \"yo quiero cocacola\",\n",
        "          \"yo quiero agua y un agua\",\n",
        "          \"yo no quiero vino\",\n",
        "          \"yo quiero un entrecot\"]"
      ],
      "metadata": {
        "id": "9EkfSTy3pxsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a tokenizar cada una de las frases, para trabajar comodamente más adelante:"
      ],
      "metadata": {
        "id": "V7yGud8qqP8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_tok = [__________________________ for i in corpus]  #Corpus conocido y sencillo, podemos dividir palabras con split()\n",
        "corpus_tok"
      ],
      "metadata": {
        "id": "fp3fAT8ppwl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words\n",
        "Como hemos visto anterioremente, Bag Of Words es uno de los modelos de representación de texto más intuitivos. Para implementarlo, se construye un vocabulario de tamaño N con los tokens únicos del corpus de trabajo, para despues representar cada documento con un vector del mismo tamaño en el que cada elemento será el nº de veces que aparece el token en cuestión en el documento.\n",
        "\n"
      ],
      "metadata": {
        "id": "FhnCPAKiqFLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar, construimos el vocabulario que se utilizará para representar nuestro corpus:"
      ],
      "metadata": {
        "id": "2eQnwDb5qjA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El vocabulario de nuestro corpus son los tokens únicos utilizados en nuestro conjunto de documentos\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "# * operador de desempaquetado: concatenar todos los sublistas (o secuencias) de corpus_tok en un solo iterador plano, que puedes convertir en una lista si quieres\n",
        "all_tokens = itertools.chain(*corpus_tok)\n",
        "vocab = sorted(set(all_tokens))\n",
        "\n",
        "print(\"Nuestro vocabulario contiene {} tokens. Que son: {}\".format(len(vocab),vocab))"
      ],
      "metadata": {
        "id": "CAsGf7eOqI-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada uno de los documentos del corpus estará representado por un vector de 9 elementos. El elemento 0 representará la presencia del token agua en el documento, el elemento 1 representará la presencia de \"cocacola\" en el documento... etc\n",
        "\n",
        "A continuación, vamos a construir esos vectores:"
      ],
      "metadata": {
        "id": "ox-OKbnNraUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "# Creamos un vector de salida\n",
        "doc_vectors = list()\n",
        "# Iteramos en cada uno de los documentos del corpus\n",
        "for doc in corpus_tok:\n",
        "  # Contamos las ocurrencias de cada uno de los tokens. Counter devuelve un diccionario\n",
        "  counter_doc = __________________ # collections.Counter\n",
        "  # Creamos el vector que representará al documento\n",
        "  doc_vec = list()\n",
        "  # Iteramos por los términos del voabulario\n",
        "  for word in vocab:\n",
        "    # Si el término está en el diccionario de conteo, introducimos el valor en\n",
        "    # esa posición del vector. Si no está, introducimos un 0\n",
        "    if word in counter_doc:\n",
        "      doc_vec.append(_____________________) #counter_doc[word]\n",
        "    else:\n",
        "      doc_vec.append(0)\n",
        "  # Añadimos al vector de salida el vector del documento\n",
        "  doc_vectors.append(doc_vec)"
      ],
      "metadata": {
        "id": "0J0ven0arZpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for frase, vector in zip(corpus, doc_vectors):\n",
        "  print(\"La frase '{}' está representada por el vector {}\".format(frase, vector))"
      ],
      "metadata": {
        "id": "Lg7a8t2As-61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En Bag Of Words:\n",
        "\n",
        "- Los términos que aparecen varias veces en el documento tienen un número mayor, lo que se puede traducir en que esos términos (caracteristicas) tienen un mayor peso en el documento.\n",
        "- Si un término ocurre muchas veces en todos los documentos, tendrá gran importancia en cada uno de ellos, a pesar de que no será útil para clasificar o agrupar textos.\n",
        "\n",
        "\n",
        "El método de representación TF-IDF intenta precisamente  compensar este efecto mediante la aplicación de una penalización a palabras comunes en muchos documentos."
      ],
      "metadata": {
        "id": "Xi8aGcOouXTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF  (intuición)\n"
      ],
      "metadata": {
        "id": "fdk6YyUjvbHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar los cáculos de forma más eficiente generaremos un diccionario que asocie a cada una de las palabras de nuestro corpus un índice numérico.\n"
      ],
      "metadata": {
        "id": "ct4pM7F6RfZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para calcular TF-IDF realizaremos los siguientes pasos:\n",
        "\n",
        "1. Calcular el Term Frequency (TF). Que se calcula como:\n",
        "  $$TF(w) = n_{w-in-d} / N_d$$\n",
        "\n",
        "Anteriormente hemos calculado para cada documento el número de veces que aparecía cada token del vocabulario. Para calcular el TF bastaría con dividir cada uno de los elementos del vector con la suma total de tokens de cada documento.\n"
      ],
      "metadata": {
        "id": "PR2pB-QDzfzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Calcular el componente Inverse-Document Frequency (IDF). Que se calcula como:\n",
        "$$ IDF(w) = ln(N/n_w)$$\n",
        "\n",
        "donde n_w  es el número de documentos que contiene el token w y N es el número total de documentos."
      ],
      "metadata": {
        "id": "q832rwMH0klF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar generamos una función que permita calcular el número de vees que un token aparece en un documento d enuestro corpus."
      ],
      "metadata": {
        "id": "7xZDRIuTUZAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despues, uniríamos ambas expresiones para calcular el TF-IDF de una frase (las siguientes celdas son de pseudocódigo):"
      ],
      "metadata": {
        "id": "HVrOHtSZUtJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "\n",
        "def tf_idf_calc(doc, N_corpus):\n",
        "  # Generamos vector salida\n",
        "  tf_idf_vec = np.zeros((len(vocab),))\n",
        "  for word in doc:\n",
        "    # Calculamoes el valor tf para el token en cuestion\n",
        "    tf = TF_calc(doc, word)\n",
        "    # Calculamos el valor idf para el token en cuestion\n",
        "    idf = idf_calc(doc, N_corpus)\n",
        "\n",
        "    tf_idf = tf * idf\n",
        "    tf_idf_vec[index_dict[word]] = tf_idf\n",
        "  return tf_idf_vec\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mCCiH-AyTpns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y lo aplicaríamos a todo el corpus:"
      ],
      "metadata": {
        "id": "PKV7tHzHU_5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "```python\n",
        "#TF-IDF Encoded text corpus\n",
        "tf_idf_result = []\n",
        "for sent in corpus_tok:\n",
        "    vec = tf_idf_calc(sent,len(corpus_tok))\n",
        "    tf_idf_result.append(vec)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Z_02B0l4UCAX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvFyVzubOmJM"
      },
      "source": [
        "## 2. Vectorización con scikit-learn\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aunque hemos mostrado una manera de aplicar el BoW y Tf-IDF a un corpus de documentos. Existen varias modificaciones que pueden mejorar los resultados (modificaciones en el cálculo de la componente IDF, por ejemplo). Además, las librerías de ML están preapradas para transformar documentos de forma más inmedianta, eficiente e incorporando funciones que pueden ser de utilidad\n",
        "\n",
        "\n",
        "En scikit-learn podemos utilizar distintas funciones para obtener el vocabulario de un corpus de documentos. Ambas están presentes dentro del módulo feature_extraction.text y son [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) y  [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer).\n",
        "Vamos a crear un objeto con cada una de esas clases para introducir nuestro corpus y extraer el vocabulario."
      ],
      "metadata": {
        "id": "Hfx5yjcoYOPA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJl8IO50AKBp"
      },
      "source": [
        "\n",
        "**Tanto las funciones CountVectorizer como TfidfVectorizer cuentan con muchos parámetros personalizables. Algunos de los más relevantes son:**\n",
        "\n",
        "\n",
        "*   *strip_accents*: Elimina los acentos en codificación ascii o unicode. Por defecto es None. Es preferible hacer una gestión de acentos previas.\n",
        "*   *lowercase*: Transforma todos los caracteres a minúsculas antes de hacer la tokenización.\n",
        "*   *tokenizer*: Utiliza un tokenizador específico. Se puede utilizar una de NLTK o de Spacy(computacionalmente menos eficiente).\n",
        "*   *stop_words*: Si se pone el valor \"english\" eliminara la lista de stop_words definida en scikit-learn. Se puede utilizar la lista de stopwords de otras librerías o definir unas propias.\n",
        "*   *ngram_range*: Cálculo de n-gramas en el proceso. Mediante la tubla (min_n, max_n) se pueden incorporar n-gramas al cálculo de la matriz tfidf.\n",
        "*   *max_df*: Valor por defecto 1.  Ignora los tokens (o n-gramas) que aparecen en más del X % de documentos cuando es menor de 1. Si max_df es mayor que uno se ignorarán los términos que aparecen en más de X documentos.\n",
        "*   *min_df*: Valor por defecto 1. Ignora los tokens que aparecen  en menos del X % de los documentos. Siendo X el valor de X. (0.01 = 1%, por ejemplo)\n",
        "*   *max_features*: Máximas características que devuelve la función TfidfVectorizer. Valor mayor que 1. Representa las caracaterísticas más importantes (las más repetidas o comunes) Esto es muy interesante para no sobreentrenar el sistema.\n",
        "*   *norm*: Valores \"l1\" y \"l2\", por defecto \"l2\". Normaliza los valores entre 0 y 1.\n",
        "*   *use_idf*: Habilita el uso del inverse-document frequency en la función. Por defecto es True.\n",
        "*   *smooth_idf*: Suaviza los pesos de IDF sumando una unidad a cada frecuencia. Es muy importante para evitar divisiones por cero.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiDKf5zHT-Rm"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "# Creamos los objetos\n",
        "count_vectorizer = CountVectorizer(________________)\n",
        "tfidf_vectorizer = TfidfVectorizer(________________) # norm=None, smooth_idf=False Re-escribimos los valores por defecto para tener el tf-idf básico\n",
        "\n",
        "# Hacemos Fit con nuestro corpus\n",
        "count_data = ___________________   # count_vectorizer.fit(corpus)\n",
        "tfidf_data = _______________   # tfidf_vectorizer.fit(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tUVaA_-UItk"
      },
      "source": [
        "# Obtenemos los vocabularios de dos formas:\n",
        "print(\"COUNT VECTORIZER\")\n",
        "print(\"Obtenemos el vocabulario en si mismo como una lista\")\n",
        "print(count_data.get_feature_names_out())\n",
        "print(\"Numero de características:\")\n",
        "print(len(count_data.get_feature_names_out()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Xi6tt1DH-0"
      },
      "source": [
        "# Obviamente, dado que hemos utilizado el mismo corpus, obtenemos el mismo resultado con el tfidf_vectorizer.\n",
        "print(\"\\n\\n TF-IDF VECTORIZER\")\n",
        "print(\"Obtenemos el vocabulario en si mismo como una lista\")\n",
        "print(tfidf_data.get_feature_names_out())\n",
        "print(\"Numero de características:\")\n",
        "print(len(tfidf_data.get_feature_names_out()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQabDt3aDJ4-"
      },
      "source": [
        "Si imprimimos el objeto tfidf_data podemos ver la configuración de los parámetros de TfidfVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhHyIi8JUiLn"
      },
      "source": [
        "tfidf_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBUeudl0Ol8L"
      },
      "source": [
        "A continuación se muestra el resultado de transformar nuestro corpus con lso métodos de scikit-learn. El `CountVectorizer` mostrará el conteo de veces que una palabra del vocabulario está presente dentro del documento, el `TfidfVectorizer` mostrará el resultado con la métrica TF-IDF mostrada en los apuntes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7BGfa8UAEvE"
      },
      "source": [
        "# Resultado del CountVectorizer\n",
        "count_data_result = count_data.transform(corpus).toarray()\n",
        "print(count_data_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnvyxz32DdUG"
      },
      "source": [
        "# Resultado del TfidfVectorizer\n",
        "tfidf_data_result = tfidf_data.______________.toarray()  # transform(corpus)\n",
        "print(tfidf_data_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xm5fMDoY-nR"
      },
      "source": [
        "Vamos a mostrar los resultados con seaborn para que se vean mejor. Importante mencionar que esto se puede hacer cuando el vocabulario es muy reducido, si no podría ocasionar problemas en la memoria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cck6bfUBZBrb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(16, 6))\n",
        "# Figura CountVectorizer\n",
        "sns.heatmap(count_data_result, annot=True,cbar=False,\n",
        "            xticklabels=count_data.get_feature_names_out(),\n",
        "            yticklabels = [\"Frase 1\", \"Frase 2\", \"Frase 3\",\"Frase 4\", \"Frase 5\"])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4viXEKmD1FT"
      },
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "sns.heatmap(tfidf_data_result, annot=True,cbar=False,\n",
        "            xticklabels=tfidf_data.get_feature_names_out(),\n",
        "            yticklabels = [\"Frase 1\", \"Frase 2\", \"Frase 3\",\"Frase 4\", \"Frase 5\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFJMRxoQNK4J"
      },
      "source": [
        "### Uso de preprocesadores externos a scikit-learn\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vov1rGLIDjr9"
      },
      "source": [
        "En primer lugar, dado que vamos a utilizar spacy, instalaremos la librería y el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZshEoLThWJDt"
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNiPnKMIDqVA"
      },
      "source": [
        "Importamos las librerías y creamos el objeto \"nlp\" para procesar los textos\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQLM1Ja1Wtak"
      },
      "source": [
        "import spacy\n",
        "import es_core_news_sm\n",
        "nlp = _________________ #es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YKq_qpdDxHA"
      },
      "source": [
        "Guardamos las stop words de Spacy en una variable llamada \"stop_words\". También cogemos los tokens considerados símbolos de punctuación en la variable \"punctuations\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_Rnj35hXCH_"
      },
      "source": [
        "import string\n",
        "spacy_stopwords = spacy.lang.es.stop_words.STOP_WORDS\n",
        "stop_words = spacy_stopwords\n",
        "punctuations=string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NFUlkdsEBUu"
      },
      "source": [
        "Generamos una función \"spacy_tokenizer\" que:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6pGl_YBO7uZ"
      },
      "source": [
        "def spacy_tokenizer(sentence):\n",
        "    # Pasamos la frase por el objeto nlp para procesarla\n",
        "    mytokens = nlp(sentence)\n",
        "\n",
        "    # Lematizamos los tokens y los convertimos  a minusculas\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Quitamos las stopwords y los signos de puntuacion\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # devolver una lsita de tokens\n",
        "    return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzZbGtGaD-Vt"
      },
      "source": [
        "Utilizamos esa función como tokenizador en TfidfVectorizer:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niPHblSRO7jM"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(norm=None, smooth_idf=False, tokenizer = spacy_tokenizer) # Re-escribimos los valores por defecto para tener el tf-idf básico\n",
        "tfidf_data = tfidf_vectorizer.fit(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjKhD6FJEY7G"
      },
      "source": [
        "print(\"\\n\\n TF-IDF VECTORIZER\")\n",
        "print(\"Obtenemos el vocabulario en si mismo como una lista\")\n",
        "print(tfidf_data._________________)\n",
        "print(\"Numero de características:\")\n",
        "print(len(tfidf_data.____________________))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70q1LsT6BnQ_"
      },
      "source": [
        "**Ejercicio**:\n",
        "\n",
        "Queremos transformar nuestro dataset de noticias con las siguientes especificaciones:\n",
        " - Transformar los primeros 3000 documentos\n",
        " - Se utilice idf y el valor de norm por defecto.\n",
        " - Se utilice la función de preprocesado de spacy utilizada anteriormente\n",
        " - Se consideren unigramas, bigramas, trigramas\n",
        " - El vectorizador no debe considerar los elementos que aparezcan en menos del 5% de documentos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdgVDoaZBmdI"
      },
      "source": [
        "# Dataset de noticias\n",
        "!wget \"https://github.com/luisgasco/ntic_master_datos/raw/main/datasets/news_summary.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0AlbRlGCzBp"
      },
      "source": [
        "import pandas as pd\n",
        "news_summary = pd.read_csv('../content/news_summary.csv', encoding='latin-1')\n",
        "news_subset = news_summary[\"text\"].to_list()[0:3000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "stop_words = spacy_stopwords\n",
        "punctuations=string.punctuation"
      ],
      "metadata": {
        "id": "6usk35QdQRni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZbmmDHR3RlB"
      },
      "source": [
        "def spacy_tokenizer(sentence):\n",
        "    # Pasamos la frase por el objeto nlp para procesarla\n",
        "    mytokens = nlp(sentence)\n",
        "\n",
        "    # Lematizamos los tokens y los convertimos  a minusculas\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Quitamos las stopwords y los signos de puntuacion\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # Quitamos números:\n",
        "    mytokens = ['' if word.isdigit() else word for word in mytokens  ]\n",
        "    # devolver una lsita de tokens\n",
        "    return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa7l5c_51DpI"
      },
      "source": [
        "tfidf_vect = TfidfVectorizer(______________________)\n",
        "tfidf_data = tfidf_vect.fit(____________________)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_data_t = tfidf_data.transform(news_subset)"
      ],
      "metadata": {
        "id": "YLN1sHqMbIdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_data_t"
      ],
      "metadata": {
        "id": "D6V9RL6H3KKS",
        "outputId": "2c6ccd86-d1a9-4be1-d1cd-b70677895bae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3000x56 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 14828 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYkkGStl2dRk"
      },
      "source": [
        "tfidf_data.get_feature_names_out()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. [EXTRA] - Diferencias entre BoW y TF-IDF\n"
      ],
      "metadata": {
        "id": "WCrHf9PfL4rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasta el momento hemos visto diferente maneras de realizar la representación de documentos utilizando el método BoW y TF-IDF.\n",
        "Sin embargo, ¿Qué implica realmente utilizar un método u otro?\n",
        "\n",
        "La clave de TF-IDF es su penalización de palabras comunes en todos los documentos. Para ver su efecto, en este apartado vamos a:\n",
        "1. Descargar un conjunto de documentos de wikipedia\n",
        "2. Preprocesarlos\n",
        "3. Mostrar los 10 términos más importantes para BoW y Tf-IDF, y ver las diferencias"
      ],
      "metadata": {
        "id": "FOE0JENtaEAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar importanmos y descargamos las librerías que utilizaremos\n"
      ],
      "metadata": {
        "id": "hsmWpxPhbxuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia\n",
        "import wikipedia\n",
        "import spacy\n",
        "import collections\n",
        "import math\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "import string\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "stop_words = spacy_stopwords\n",
        "punctuations=string.punctuation\n",
        "RE_WHITESPACE = re.compile(r\"\\s+\")\n"
      ],
      "metadata": {
        "id": "Mr63VVHsMZCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una función de procesado similar a la anteriormente definida. En este caso eliminarmos los números (no los sustitumos por un caracter vacío). Además, eliminamos posibles espacios extras despues del proceso de tokenización"
      ],
      "metadata": {
        "id": "HhaSP7W9cC0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spacy_tokenizer(sentence):\n",
        "    # Pasamos la frase por el objeto nlp para procesarla\n",
        "    mytokens = nlp(sentence)\n",
        "\n",
        "    mytokens = [ word.text.lower()   if word.pos_ == 'PRON' or word.lemma_ == '-PRON-' else word.lemma_.lower() for word in mytokens]\n",
        "\n",
        "    # Lematizamos los tokens y los convertimos  a minusculas\n",
        " #   mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Quitamos las stopwords y los signos de puntuacion\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # Quitamos números:\n",
        "    mytokens = [word  for word in mytokens if not word.isdigit()]\n",
        "    # Remove extra spaces\n",
        "    mytokens = [token.strip() for token in mytokens]\n",
        "    # devolver una lsita de tokens\n",
        "    return mytokens\n"
      ],
      "metadata": {
        "id": "FctYfGP7MA99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a descargar el texto de descripción de varios guitarristas históricos. Para ello definiremos una lista de strings con sus nombres y los obtendremos utilizando la función \"page\" de la librería wikipedia.\n"
      ],
      "metadata": {
        "id": "9MJ44YYJcKb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pages =  [\n",
        "  \"Brian May\",\"jimmy page\",\"eddie van halen\",\"david gilmour\",\n",
        "  \"Jeff Beck\",\"mark knopfler\",\"Billy Gibbons\",\"Carlos Santana\",\n",
        "  \"Stevie Ray Vaughan\",\"BB king\",\"Buddy Guy\",\"Albert King\",\"Rory Gallagher\",\n",
        "  \"Joe Satriani\", \"jimi hendrix\",\"George Harrison\"\n",
        "    ]\n",
        "documentes = [RE_WHITESPACE.sub(\" \",wikipedia.page(page, auto_suggest=False).content).strip() for page in pages]"
      ],
      "metadata": {
        "id": "Zv4V4SbkaLfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculamos y entrenamos vectorizadores TFIDF y BoW (CountVectorizer) para el corpus descargado y preprocesado:"
      ],
      "metadata": {
        "id": "JIoXIw21cekC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vect = TfidfVectorizer(lowercase = False, stop_words = None, use_idf = True, smooth_idf = False,\n",
        "                             norm = \"l2\", tokenizer=spacy_tokenizer)\n",
        "count_vect = CountVectorizer(lowercase = False, stop_words = None, tokenizer=spacy_tokenizer)\n",
        "\n",
        "tfidf_data = tfidf_vect.fit(documentes)\n",
        "count_data = count_vect.fit(documentes)"
      ],
      "metadata": {
        "id": "kKH3O6NwMris",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c15d55a-a828-42a1-8959-d5df157f8566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos funciones para obtener las palabras más importantes del vectorizador:\n"
      ],
      "metadata": {
        "id": "qSNTuC2GcxzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_feature_names = np.array(tfidf_data.get_feature_names_out())\n",
        "count_feature_names = np.array(count_data.get_feature_names_out())\n",
        "\n",
        "def get_top_vect_words(response, top_n=2,feature_name_array=tfidf_feature_names):\n",
        "  # De la respuesta del vectorizador, cogemos los datos en bruto del array, los ordenamos\n",
        "  # de mayor a menor y cogemos los índices de los top_n terminos. Que se seleccionarán del\n",
        "  # los nombres de las caracteristicas (tokens) del vectorizador.\n",
        "    sorted_nzs = np.argsort(response.data)[:-(top_n+1):-1]\n",
        "    return feature_name_array[response.indices[sorted_nzs]]\n"
      ],
      "metadata": {
        "id": "armdlrKJMWoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos está función para mostrar los 10 términos más importantes de un documento vectorizado con TF-IDF y BoW para ver sus diferencias."
      ],
      "metadata": {
        "id": "FaBwKqfpdTBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_doc_results(doc,top_n):\n",
        "  responses_tfidf = tfidf_vect.transform([doc])\n",
        "  responses_count = count_vect.transform([doc])\n",
        "  most_frequent = list(get_top_vect_words(responses_count,top_n,count_feature_names))\n",
        "  tfidf = list(get_top_vect_words(responses_tfidf,top_n,tfidf_feature_names))\n",
        "  print(\"Tokens más frecuentes: {}\".format(most_frequent))\n",
        "  print(\"Tokens con mayor TF-IDF: {}\".format(tfidf))"
      ],
      "metadata": {
        "id": "X1fO7JoYc7-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(pages)):\n",
        "  print(pages[i])\n",
        "  show_doc_results(documentes[i],10)"
      ],
      "metadata": {
        "id": "Tf7C8FC6gH-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f9400f-5dd3-4373-a48f-53cd21705271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brian May\n",
            "Tokens más frecuentes: ['queen', 'guitar', 'album', 'use', 'band', 'rock', 'song', 'work', 'perform', 'tour']\n",
            "Tokens con mayor TF-IDF: ['queen', 'guitar', 'badger', 'mercury', 'animal', 'lambert', 'album', 'use', 'band', 'imperial']\n",
            "jimmy page\n",
            "Tokens más frecuentes: ['page', 'zeppelin', 'guitar', 'play', 'led', 'album', 'use', 'record', 'session', 'band']\n",
            "Tokens con mayor TF-IDF: ['page', 'zeppelin', 'led', 'guitar', 'play', 'plant', 'album', 'yardbirds', 'use', 'record']\n",
            "eddie van halen\n",
            "Tokens más frecuentes: ['van', 'halen', 'guitar', 'eddie', 'band', 'later', 'play', 'use', 'album', 'solo']\n",
            "Tokens con mayor TF-IDF: ['halen', 'van', 'guitar', 'eddie', 'evh', 'wolfgang', 'bertinelli', 'gene', 'valerie', 'pasadena']\n",
            "david gilmour\n",
            "Tokens más frecuentes: ['gilmour', 'guitar', '–', 'pink', 'vocal', 'floyd', 'album', 'david', 'release', 'use']\n",
            "Tokens con mayor TF-IDF: ['gilmour', 'pink', 'floyd', '–', 'guitar', 'vocal', 'barrett', 'steel', 'strat', 'album']\n",
            "Jeff Beck\n",
            "Tokens más frecuentes: ['beck', 'album', '–', 'rock', 'guitar', 'record', 'jeff', 'release', 'instrumental', 'best']\n",
            "Tokens con mayor TF-IDF: ['beck', '–', 'album', 'bogert', 'appice', 'rock', 'instrumental', 'jeff', 'best', 'guitar']\n",
            "mark knopfler\n",
            "Tokens más frecuentes: ['knopfler', 'album', 'straits', 'dire', 'release', 'tour', 'music', 'band', 'mark', 'solo']\n",
            "Tokens con mayor TF-IDF: ['knopfler', 'dire', 'straits', 'album', 'newcastle', 'illsley', 'release', 'country', 'fletcher', 'mark']\n",
            "Billy Gibbons\n",
            "Tokens más frecuentes: ['gibbon', 'guitar', 'gibbons', 'album', 'play', 'zz', 'song', 'angela', 'hodgins', 'string']\n",
            "Tokens con mayor TF-IDF: ['gibbon', 'gibbons', 'zz', 'hodgins', 'guitar', 'angela', 'sidewalks', 'album', 'moving', 'bfg']\n",
            "Carlos Santana\n",
            "Tokens más frecuentes: ['santana', 'album', 'band', 'guitar', 'carlos', 'record', 'year', 'use', 'release', 'rock']\n",
            "Tokens con mayor TF-IDF: ['santana', 'carlos', 'album', 'band', 'prs', 'latin', 'guitar', 'supernatural', 'rolie', 'chinmoy']\n",
            "Stevie Ray Vaughan\n",
            "Tokens más frecuentes: ['vaughan', 'guitar', 'play', 'album', 'double', 'trouble', 'stevie', 'band', 'include', 'texas']\n",
            "Tokens con mayor TF-IDF: ['vaughan', 'trouble', 'stevie', 'guitar', 'texas', 'double', 'play', 'lenny', 'shannon', 'album']\n",
            "BB king\n",
            "Tokens más frecuentes: ['king', 'blues', 'b.b.', 'music', 'play', 'mississippi', 'guitar', 'later', 'hall', 'award']\n",
            "Tokens con mayor TF-IDF: ['king', 'mississippi', 'b.b.', 'blues', 'memphis', 'indianola', 'music', 'beale', 'play', 'guitar']\n",
            "Buddy Guy\n",
            "Tokens más frecuentes: ['guy', 'guitar', 'blue', 'buddy', 'polka', 'dot', 'play', 'chess', 'record', 'clapton']\n",
            "Tokens con mayor TF-IDF: ['guy', 'polka', 'dot', 'buddy', 'chess', 'guitar', 'blue', 'chicago', 'louisiana', 'lie']\n",
            "Albert King\n",
            "Tokens más frecuentes: ['king', 'album', 'albert', 'blues', 'guitar', 'release', 'record', 'blue', 'play', 'know']\n",
            "Tokens con mayor TF-IDF: ['king', 'blues', 'stax', 'albert', 'album', 'c', 'e', 'guitar', 'louis', 'indianola']\n",
            "Rory Gallagher\n",
            "Tokens más frecuentes: ['gallagher', 'rory', 'play', 'guitar', 'album', 'band', 'guitarist', 'blue', 'year', 'live']\n",
            "Tokens con mayor TF-IDF: ['gallagher', 'rory', 'taste', 'cork', 'dónal', 'mcavoy', 'ireland', 'play', 'guitar', 'ballyshannon']\n",
            "Joe Satriani\n",
            "Tokens más frecuentes: ['satriani', 'album', 'guitar', 'release', 'use', 'song', 'feature', 'joe', 'guitarist', 'music']\n",
            "Tokens con mayor TF-IDF: ['satriani', 'chickenfoot', 'album', 'guitar', 'release', 'js', 'use', 'joe', 'peavey', 'g3']\n",
            "jimi hendrix\n",
            "Tokens más frecuentes: ['hendrix', 'guitar', 'experience', 'play', 'band', 'use', 'rock', 'jimi', 'music', 'year']\n",
            "Tokens con mayor TF-IDF: ['hendrix', 'chandler', 'experience', 'redding', 'guitar', 'cox', 'mitchell', 'play', 'ladyland', 'band']\n",
            "George Harrison\n",
            "Tokens más frecuentes: ['harrison', 'album', 'song', 'guitar', 'music', 'beatle', 'george', 'release', 'include', 'lennon']\n",
            "Tokens con mayor TF-IDF: ['harrison', 'beatle', 'beatles', 'shankar', 'album', 'lennon', 'indian', 'song', 'mccartney', 'guitar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observar diferencias de palabras muy comunes en documentos de guitarristas como \"guitar\", \"release\",\"play\" o \"album\". En BoW aparecen como términos muy importantes porque son comunes en este tipo de documentos. Sin embargo, al calcular el TF-IDF, como estos términos aparecen en muchos documentos su importancia se ve disminuida.\n"
      ],
      "metadata": {
        "id": "v_1diBNEdfn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fET1CgwbLS5s"
      }
    }
  ]
}