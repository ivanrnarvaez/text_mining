{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "M5MhVlxlKxJF",
        "outputId": "b02cc34f-0bf0-4fdd-d444-cc6bd5210bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.3 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install contractions\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install fsspec==2023.9.2\n",
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5dffd1b"
      },
      "source": [
        "\n",
        "⚙️ **Requerimientos importantes sobre el ejercicio**\n",
        "\n",
        "- El notebook debe ejecutarse **de principio a fin sin intervención manual**.\n",
        "- Si utilizas librerías que no están incluidas por defecto en Google Colab, **asegúrate de instalarlas dentro del notebook** (por ejemplo: `!pip install ...`).\n",
        "\n",
        "- Algunas celdas incluyen identificadores especiales que indican ciertas normas que **debes** respetar:\n",
        " - `#NO-MODIFY: DATA LOAD`  \n",
        "    🔒 **No modifiques** el contenido de esta celda.\n",
        "\n",
        "  - `#NO-MODIFY: VARIABLE NAME`  \n",
        "    ✏️ Puedes modificar o añadir información **dentro de la celda**, pero **sin cambiar el nombre de la variable asignada**. No incluyas más variables de las existentes en la celda.\n",
        "\n",
        "  - `#MODIFY: ADD INFO TO SOLVE FUNCTION`  \n",
        "    🔧 Puedes modificar el **interior de la función** para resolver la tarea, pero **no cambies su nombre, la cabecera ni el `return`**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w1C3ZBXHL0b"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WTXfHYTHQmo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5xkvK9cHSse"
      },
      "outputs": [],
      "source": [
        "# Add your imports here\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "import contractions\n",
        "from tqdm.autonotebook import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1KKTZUcH_xk"
      },
      "source": [
        "# 🔍 Ejercicio1: Detección de profesiones en tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enunciado"
      ],
      "metadata": {
        "id": "gLDLR2bEHk2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejercicio vamos a trabajar con un conjunto de datos procedente de medios sociales online.\n",
        "\n",
        "Utilizaremos un subconjunto de los datos de la tarea 1 del shared task [**ProfNER**](https://temu.bsc.es/smm4h-spanish), centrada en la detección de menciones a profesiones en tweets publicados durante la pandemia del COVID-19. El objetivo original de la tarea era analizar que profesiones podrían haber sido especialmente vulnerables en el contexto de la crisis sanitaria.\n",
        "\n",
        "Para simplificar el ejercicio, he preparado una versión reducida del dataset original. Tu tarea será entrenar un clasificador binario basado en la arquitectura Transformers, que, dado un tweet, determine si contiene una mención explícita a una profesión (etiqueta `1`) o no (etiqueta `0`).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6kx1_gwPQGcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ **Objetivos del ejercicio**\n",
        "\n",
        "A lo largo de este notebook, completarás las siguientes etapas para construir un clasificador de menciones a profesiones en tweets:\n",
        "\n",
        "1. **Análisis Exploratorio de Datos (EDA)**: Calcular estadísticas básicas del conjunto de datos (como el número de ejemplos del training set, la distribución de clases del dataset, la longitud media de los textos) o crear visualizaciones para cmprender mejor el contenido de los documentos usando wordclouds o histogramas.\n",
        "\n",
        "2. **Selección y justificación del modelo**: Elegir un modelo del Hub de Huggingface adecuado para los datos con los que se va a trabajar y el tipo de tarea a desarrollar.\n",
        "\n",
        "3. **Entrenamiento del clasificador**: Entrenar el modelo de forma reproducible y evaluar su rendimiento sobreel conjunto de datos de validación, incluyendo un classification score y matriz de confusion\n",
        "\n",
        "4. **Generación de predicciones sobre el conjunte de test**: Aplicar el modelo entrenado al conjunto de test, y guardar las predicciones en un archivo `.tsv` de 2 columnas `id` y `label` separadas por tabulador"
      ],
      "metadata": {
        "id": "3VHMEt0x-7UO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📝 **Criterios de Evaluación**\n",
        "\n",
        "Tu trabajo será evaluado según los siguientes criterios:\n",
        "\n",
        "| Criterio                                            | Peso  |\n",
        "|-----------------------------------------------------|--------|\n",
        "| 🔍 Análisis exploratorio y preprocesamiento         | 20%   |\n",
        "| 🤖 Selección y justificación del modelo             | 25%   |\n",
        "| 📁 Formato y validez del archivo de predicciones    | 5%    |\n",
        "| ⚙️ Ejecución correcta del notebook (sin intervención) | 10%   |\n",
        "| 📈 Rendimiento del modelo sobre el conjunto de test | 30%   |\n",
        "| ✍️ Claridad y calidad de las explicaciones          | 10%   |\n",
        "\n",
        "\n",
        "\n",
        "🔔 **Nota importante:**\n",
        "\n",
        "> El rendimiento del modelo se evaluará utilizando métricas estándar como el **F1-score** sobre el conjunto de test.\n",
        "\n",
        "> El archivo de predicciones debe respetar **estrictamente** el formato solicitado (`id` y `label`, separados por tabulador y con extensión `.tsv`).  \n",
        "  ❗ Si el archivo no cumple con este formato, **el ejercicio no podrá ser evaluado en esa sección**.\n",
        "\n",
        "> El/la estudiante con el **mayor F1-score** obtendrá la puntuación máxima en el apartado de rendimiento. El resto de calificaciones se ajustarán de forma proporcional al mejor resultado\n"
      ],
      "metadata": {
        "id": "VPaXLRNeAElo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "⚙️ **Requerimientos y reglas**\n",
        "\n",
        "- El notebook debe ejecutarse **de principio a fin sin intervención manual**.\n",
        "- Si utilizas librerías que no están incluidas por defecto en Google Colab, **asegúrate de instalarlas dentro del notebook** (por ejemplo: `!pip install ...`).\n",
        "\n",
        "- Algunas celdas incluyen identificadores especiales que indican ciertas normas que **debes** respetar:\n",
        " - `#NO-MODIFY: DATA LOAD`  \n",
        "    🔒 **No modifiques** el contenido de esta celda.\n",
        "\n",
        "  - `#NO-MODIFY: VARIABLE NAME`  \n",
        "    ✏️ Puedes modificar o añadir información **dentro de la celda**, pero **sin cambiar el nombre de la variable asignada**. No incluyas más variables de las existentes en la celda.\n",
        "\n",
        "  - `#MODIFY: ADD INFO TO SOLVE FUNCTION`  \n",
        "    🔧 Puedes modificar el **interior de la función** para resolver la tarea, pero **no cambies su nombre, la cabecera ni el `return`**.\n"
      ],
      "metadata": {
        "id": "3M19bykxA0ZP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj9IcJEwIB91"
      },
      "source": [
        "# Tu resolución (rellena las celdas marcadas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOXm4JVwIElL"
      },
      "source": [
        "## Obtención de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargamos los datos del [repositorio de Huggingface](https://huggingface.co/datasets/luisgasco/profner_classification_master)."
      ],
      "metadata": {
        "id": "gtOwX5HKCSfg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aiOIGd78IG3y"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: DATA LOAD\n",
        "from datasets import load_dataset, Dataset, DatasetDict, ClassLabel\n",
        "dataset = load_dataset(\"luisgasco/profner_classification_master\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El dataset contiene tres subsets:\n",
        "- **train** y **validation**: Contienen el identificador del tweet, el texto, y su etiqueta, que podrá tener valor 1, si contiene una mención de una profesión; o valor 0, si no contiene una mención de una profesión.\n",
        "- **test**: El test set tambiíen contiene la información de label por un requerimiento de Huggingface, pero el contenido de esta variable es siempre \"-1\". Es decir que deberéis predecir nuevas etiquetas una vez hayáis entrenado el modelo utilizando el train y el validation set."
      ],
      "metadata": {
        "id": "nY-vjg88CfpW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "163nmdUnIG-P"
      },
      "source": [
        "## Análisis exploratorio de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para hacer el análisis exploratorio de datos, transformamos cada subset a un pandas dataframe para mayor comodidad."
      ],
      "metadata": {
        "id": "umz-kP7yDDkL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkphOXpCIhqj"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: DATA LOAD\n",
        "dataset_train_df = dataset[\"train\"].to_pandas()\n",
        "dataset_val_df = dataset[\"validation\"].to_pandas()\n",
        "dataset_test_df = dataset[\"test\"].to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Número de documentos**\n",
        "\n",
        "Obten con la función `get_num_docs_evaluation()` el número de documentos del dataset de training y validation.\n",
        "\n",
        "> Recuerda incorporar la información para el cálculo dentro del a siguiente celda, sin modificar los atributos de entrada ni de salida de la función, ni su nombre."
      ],
      "metadata": {
        "id": "kJ4lLzjODJTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_df.to_csv('train.csv', index=False)\n",
        "dataset_val_df.head()\n",
        "# dataset_test_df.head()\n",
        "#dataset_train_df[dataset_train_df['tweet_id'] == '1289869622187945984']"
      ],
      "metadata": {
        "id": "3VWuIgBGb_Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MODIFY: ADD INFO TO SOLVE FUNCTION\n",
        "def get_num_docs_evaluation(dataset_df):\n",
        "  # Modifica la función.\n",
        "  num_docs = len(dataset_df)\n",
        "\n",
        "  # Si se quiere obtener únicamente los que tienen textos distintos de vacío\n",
        "  # num_docs = len(dataset_df[~dataset_df['text'].isna()])\n",
        "\n",
        "  # No modifiques el return\n",
        "  return num_docs\n"
      ],
      "metadata": {
        "id": "8v9AXqgMEh0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez generada la función, puedes utilizarla posteriormente para calcular resultados y comentarlos"
      ],
      "metadata": {
        "id": "PVxQg5u_FPlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica la función\n",
        "print(f\"El numero de documentos en TRAIN son: {get_num_docs_evaluation(dataset_train_df)}\")\n",
        "print(f\"El numero de documentos en VALIDATION son: {get_num_docs_evaluation(dataset_val_df)}\")\n"
      ],
      "metadata": {
        "id": "5l82IvmOFUBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Número de documentos duplicados**\n",
        "\n",
        "Obten con la función `detect_duplicates_evaluation()` el número de documentos duplicados del dataset de training y validation.\n",
        "\n",
        "> Recuerda incorporar la información para el cálculo dentro del a siguiente celda, sin modificar los atributos de entrada ni de salida de la función, ni su nombre."
      ],
      "metadata": {
        "id": "TrxV3XkcFYhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_cviBv-IxHJ"
      },
      "outputs": [],
      "source": [
        "#MODIFY: ADD INFO TO SOLVE FUNCTION\n",
        "def detect_duplicates_evaluation(dataset_df):\n",
        "  # Modifica la función.\n",
        "\n",
        "  num_duplicates = dataset_df.duplicated(subset='text').sum()\n",
        "  # No modifiques el return\n",
        "  return num_duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez generada la función, puedes utilizarla posteriormente para calcular resultados y comentarlos"
      ],
      "metadata": {
        "id": "y_4zYMnOFjhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica la función\n",
        "print(f\"El número de documentos duplicados en TRAIN son: {detect_duplicates_evaluation(dataset_train_df)}\")\n",
        "print(f\"El número de documentos duplicados en VALIDATION son: {detect_duplicates_evaluation(dataset_val_df)}\")"
      ],
      "metadata": {
        "id": "4AgUKaVQFjhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZq8ZXzkJk0t"
      },
      "source": [
        "**Número de documentos por cada clase:**\n",
        "\n",
        "\n",
        "Obten con la función `analyse_num_labels_evaluation()` para calcular el número de documentos de cada categoría en el dataset\n",
        "\n",
        "> Recuerda incorporar la información para el cálculo dentro del a siguiente celda, sin modificar los atributos de entrada ni de salida de la función, ni su nombre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvfd9yVVJo6-"
      },
      "outputs": [],
      "source": [
        "#MODIFY: ADD INFO TO SOLVE FUNCTION\n",
        "def analyse_num_labels_evaluation(dataset_df):\n",
        "  # Modifica la función.\n",
        "  num_positives = len(dataset_df[dataset_df['label'] == 1])\n",
        "  num_negatives = len(dataset_df[dataset_df['label'] == 0])\n",
        "\n",
        "  # No modifiques el return\n",
        "  return num_positives, num_negatives"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez generada la función, puedes utilizarla posteriormente para calcular resultados y comentarlos"
      ],
      "metadata": {
        "id": "zuRURsOqGVF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica la función\n",
        "num_train_positives, num_train_negatives = analyse_num_labels_evaluation(dataset_train_df)\n",
        "num_val_positives, num_val_negatives = analyse_num_labels_evaluation(dataset_val_df)\n",
        "\n",
        "print(f\"El número de documentos en TRAIN que son positivos son: {num_train_positives} y negativos son: {num_train_negatives}\")\n",
        "print(f\"El número de documentos en VALIDATION que son positivos son: {num_val_positives} y negativos son: {num_val_negatives}\")\n"
      ],
      "metadata": {
        "id": "IUx_-_wFGVGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxXknuNvKK8f"
      },
      "source": [
        "**Distribución de la longitud de los tweet en caracteres:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_rLDRgCKU8j"
      },
      "outputs": [],
      "source": [
        "dataset_train_df['char_len'] = dataset_train_df['text'].apply(lambda x: len(x))\n",
        "\n",
        "# Importamos las librerías matplotlib y seaborn:\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig = plt.figure(figsize=(14,12))\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "sns.displot(data=dataset_train_df, x=\"char_len\", hue=\"label\", kde=True, legend=True)\n",
        "plt.legend([\"Incluye profesiones\", \"No incl. profesiones\"])\n",
        "# Definimos el título de los ejes:\n",
        "plt.xlabel('Caracteres-TRAIN', fontsize=16)\n",
        "plt.ylabel('Densidad', fontsize=16)\n",
        "\n",
        "# Finalmente mostramos el gráfico:\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_df[dataset_train_df['label'] == 1].describe()"
      ],
      "metadata": {
        "id": "UksGvhSGf4Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_df[dataset_train_df['label'] == 0].describe()"
      ],
      "metadata": {
        "id": "RpVVhqoXHJl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Aquí se observa que los tweets que contienen profesiones tienen a ser más largos que los que no contienen. En el caso de los que si contientienen profesiones, el son en promedio más grandes que los que no los contienen en un 22.36% de longuitud. El tweet mas largo es de 454 (Con profesión) y el mas corto es 10 caracteres(sin profesión). En general los textos donde se mencionan profesiones son mucho más grandes que los que no los incluyen."
      ],
      "metadata": {
        "id": "71ZYnPHYGhjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_val_df['char_len'] = dataset_val_df['text'].apply(lambda x: len(x))\n",
        "\n",
        "# Importamos las librerías matplotlib y seaborn:\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig = plt.figure(figsize=(14,12))\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "sns.displot(data=dataset_val_df, x=\"char_len\", hue=\"label\", kde=True, legend=True)\n",
        "plt.legend([\"Incluye profesiones\", \"No incl. profesiones\"])\n",
        "# Definimos el título de los ejes:\n",
        "plt.xlabel('Caracteres-VAL', fontsize=16)\n",
        "plt.ylabel('Densidad', fontsize=16)\n",
        "\n",
        "# Finalmente mostramos el gráfico:\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qFicqwFI51--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_val_df.describe()"
      ],
      "metadata": {
        "id": "P60RwVEv5_F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrtI16QKKNZq"
      },
      "source": [
        "**Análisis de contenido de los tweets**\n",
        "\n",
        "Para ello utiliza wordclouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZCi84lYK4jW"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "txt_cat0 = \",\".join(dataset_train_df[dataset_train_df['label'] == 0]['text'].to_list())\n",
        "txt_cat1 = \",\".join(dataset_train_df[dataset_train_df['label'] == 1]['text'].to_list())\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "es_stopwords = set(stopwords.words('spanish'))\n",
        "\n",
        "wordcloud = WordCloud(background_color='white',\n",
        "                      max_words=5000,\n",
        "                      contour_width=0,\n",
        "                      contour_color='plasma',\n",
        "                      stopwords=es_stopwords,\n",
        "                      normalize_plurals=True)\n",
        "\n",
        "wordcloud.generate(txt_cat0)\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud.generate(txt_cat1)\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "VXix1GpJ4_dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Se observar palabras clave como Coronavirus, Gobierno, Pandemia. Pero hay un gran presencia de textos a referencias. Después de la tokenización y limpieza se podría observar mejor lo que indica. Sin embargo es claro que en el caso donde no se meciona las profesiones, la idea clara es la presencia del virus y el confinaimiento. Mientras que en el caso de donde se menciona la profesion, la palabra sanitario, pandemia y gobierno está representadas con mayor tamaño."
      ],
      "metadata": {
        "id": "oGebKDHFIZaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SOLO PARA TRAINING\n",
        "# Utilizando el procedimiento que vimos en clase para limpiar mejor los datasets\n",
        "# y observar mejor el contenido - Solo para analisis exploratorio\n",
        "\n",
        "# Eliminar espacios\n",
        "def eliminar_espacios(text):\n",
        "    return  \" \".join(text.split())\n",
        "\n",
        "# To lower\n",
        "def texto_to_lower(text):\n",
        "  return text.lower()\n",
        "\n",
        "def replace_contraction(text):\n",
        "  return contractions.fix(text, slang=True)\n",
        "\n",
        "# Tokenizador\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "def normalizar_tokens(documento):\n",
        "    # Dividir el documento en palabras\n",
        "    palabras =  tweet_tokenizer.tokenize(documento)\n",
        "\n",
        "    # Reemplazar URLs, menciones de Twitter y números por los tokens correspondientes\n",
        "    for i in range(len(palabras)):\n",
        "        if palabras[i].startswith(\"http://\") or palabras[i].startswith(\"https://\") or palabras[i].startswith(\"www.\"):\n",
        "            palabras[i] = \"URL\"\n",
        "        elif palabras[i].startswith(\"@\"):\n",
        "            palabras[i] = \"MENTION\"\n",
        "        elif palabras[i].isdigit():\n",
        "            palabras[i] = \"NUM\"\n",
        "\n",
        "    # Unir las palabras de nuevo en un documento modificado\n",
        "    documento_modificado = ' '.join(palabras)\n",
        "\n",
        "    return documento_modificado\n",
        "\n",
        "# Lematizar\n",
        "import spacy\n",
        "nlp = spacy.load('es_core_news_md', disable=['parser', 'ner'])\n",
        "# es_core_news_md -> idioma español\n",
        "\n",
        "def lematizar_eliminacion_tokens(texto):\n",
        "    # Procesar el texto con el objeto nlp\n",
        "    doc = nlp(texto)\n",
        "\n",
        "    # Lematizar el texto\n",
        "    lemas = [token.lemma_ for token in doc]\n",
        "\n",
        "    # Eliminar símbolos de puntuación y stopwords\n",
        "    tokens_filtrados = [token for token in lemas if token.isalpha() and token.lower() not in es_stopwords]\n",
        "\n",
        "    # Unir los tokens filtrados en un nuevo texto\n",
        "    texto_procesado = ' '.join(tokens_filtrados)\n",
        "\n",
        "    return texto_procesado\n"
      ],
      "metadata": {
        "id": "v6CH_9QpW6IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_df['normalized'] = dataset_train_df['text'].progress_apply(lambda x: eliminar_espacios(x))\n",
        "dataset_train_df['normalized'] = dataset_train_df['normalized'].progress_apply(lambda x: texto_to_lower(x))\n",
        "dataset_train_df['normalized'] = dataset_train_df['normalized'].progress_apply(lambda x: replace_contraction(x))\n",
        "dataset_train_df['normalized'] = dataset_train_df['normalized'].progress_apply(lambda x: normalizar_tokens(x))\n",
        "dataset_train_df['normalized'] = dataset_train_df['normalized'].progress_apply(lambda x: lematizar_eliminacion_tokens(x))"
      ],
      "metadata": {
        "id": "mXKwwnyGWqAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_cat_norm0 = \",\".join(dataset_train_df[dataset_train_df['label'] == 0]['normalized'].to_list())\n",
        "txt_cat_norm1 = \",\".join(dataset_train_df[dataset_train_df['label'] == 1]['normalized'].to_list())\n",
        "\n",
        "# Para eliminar el ruido creadas en la depuración\n",
        "es_stopwords.update([\"URL\", \"MENTION\", \"NUM\"])\n",
        "\n",
        "wordcloud.generate(txt_cat_norm0)\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "-qEEkmxS7pYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud.generate(txt_cat_norm1)\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "o7qV1zwl746s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZUV6mSIJO1"
      },
      "source": [
        "## Tokenización"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El texto del dataset no está preparado para ser introducido en un modelo Transformers. Lleva a cabo el proceso de tokenización."
      ],
      "metadata": {
        "id": "61YNAqIAG3aq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGA4Rz7UIDUz"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test_df.head()"
      ],
      "metadata": {
        "id": "IY_ID8ARVSRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecciona un modelo apropiado para la tarea:"
      ],
      "metadata": {
        "id": "kGZStD5MHBuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Recuerda que en la siguiente celda sólo debes asignar un valor a model_name. No añadas más información en la celda."
      ],
      "metadata": {
        "id": "famSsepTHJPx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svQiqzz_Lywz"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: VARIABLE NAME\n",
        "model_name = 'FacebookAI/xlm-roberta-base'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> El modelo seleccionado es el FacebookAI/xlm-roberta-base.\n",
        "Este modelo tiene una gran cantidad des descargas (13,501,854 en el último mes), es multilenguage (por lo que español está incluido), es creado por Facebook (una companía de importancia en la industria), y tiene mucha informacion de soporte.\n",
        "Fuente:\n",
        "https://huggingface.co/FacebookAI/xlm-roberta-base"
      ],
      "metadata": {
        "id": "FTIJVHiajYtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puedes continuar con el proceso aquí:"
      ],
      "metadata": {
        "id": "lMivggEnHQqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict, ClassLabel\n",
        "dataset_train_hf = Dataset.from_pandas(dataset_train_df).rename_column(\"label\", \"labels\")\n",
        "dataset_val_hf = Dataset.from_pandas(dataset_val_df).rename_column(\"label\", \"labels\")\n",
        "dataset_test_hf = Dataset.from_pandas(dataset_test_df).rename_column(\"label\", \"labels\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LdoDvBeEMTah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_hf = dataset_train_hf.cast_column(\"labels\", ClassLabel(num_classes=2)) # 0,1"
      ],
      "metadata": {
        "id": "zkJiNKPCUHk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "   AutoConfig,\n",
        "   AutoTokenizer,\n",
        "   AutoModelForSequenceClassification\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenizar el texto\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "dataset_train_hf = dataset_train_hf.map(preprocess_function, batched=True)\n",
        "dataset_val_hf = dataset_val_hf.map(preprocess_function, batched=True)\n",
        "dataset_test_hf = dataset_test_hf.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "o7EvRUAJXjVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUel8a-FN0nB"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carga el model para ser ajustado posteriormente:"
      ],
      "metadata": {
        "id": "cOb2YTABXDv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "id2label = {0: \"NO_INCL_PROFESION\", 1: \"INCL_PROFESION\"}\n",
        "label2id = {\"NO_INCL_PROFESION\": 0, \"INCL_PROFESION\": 1}\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,  num_labels=2, id2label=id2label, label2id=label2id)\n"
      ],
      "metadata": {
        "id": "IzfR_vm5XHOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeUuTSJpN9pN"
      },
      "source": [
        "### Configuracion training_args"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configura los parámetros de entrenamiento del modelo.\n",
        "\n",
        "\n",
        ">"
      ],
      "metadata": {
        "id": "0Iy9mSLIIkju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Recuerda que en la siguiente celda sólo debes asignar atributos a la variable training_args. No añadas  otras variables en la celda"
      ],
      "metadata": {
        "id": "ucWL18iUIqme"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mexUYxXoN9v7"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: VARIABLE NAME\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"modelo_test\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.1,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        "    seed=52\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LosxnaBOIto"
      },
      "source": [
        "### Métricas de evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define las métricas de evaluación"
      ],
      "metadata": {
        "id": "Xkg3xIBBIzdT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99-S1UCeOLAH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1_score = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy_value = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    f1_score_value = f1_score.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_value,\n",
        "        \"f1_score\": f1_score_value,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iVkFUvSMDjDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EmbLYGnFDjGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KAtsuG7mDjI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YFV7ugfOMtr"
      },
      "source": [
        "### Ajuste del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lleva a cabo el ajuste del modelo:"
      ],
      "metadata": {
        "id": "62fSC9hEI3ih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OYXvp-KOQ2J"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_train_hf,\n",
        "    eval_dataset=dataset_val_hf,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2) # Podriamoshacer une arly stop\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "m9c78TrnaygB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"output_model_save\")"
      ],
      "metadata": {
        "id": "-jeyzFc2a7l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qwerqwer"
      ],
      "metadata": {
        "id": "YxuuGxNUdtr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hi38oUIOevr"
      },
      "source": [
        "## Evaluacion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez llevada a cabo el entrenamiento, realiza la evaluación del modelo."
      ],
      "metadata": {
        "id": "OPYzfaRRI-Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(dataset_test_hf)"
      ],
      "metadata": {
        "id": "rEOk5u3GXOhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9sPLjPWLMQNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Genera predicciones"
      ],
      "metadata": {
        "id": "Sf1-X0ozH1x1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genera predicciones sobre el test set. Recuerda que el archivo que generes y adjuntes al ejercicio debe tener dos columnas:\n",
        "\n",
        "\n",
        "| id         | label |\n",
        "|------------|-------|\n",
        "| 1234567890 | 1     |\n",
        "| 1234567891 | 0     |\n",
        "| 1234567892 | 0     |\n",
        "| 1234567893 | 1     |\n",
        "\n",
        "- El archivo debe estar en formato **TSV** (separado por tabuladores).\n",
        "- Debe contener exactamente **dos columnas**: `id` y `label`.\n",
        "- Es obligatorio incluir la **cabecera**.\n"
      ],
      "metadata": {
        "id": "4GyrGKTfJUIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "# Cambia esto por tu modelo\n",
        "hub_model_id = \"luisgasco/modelo_test\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(hub_model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(hub_model_id)\n",
        "\n",
        "dataset_tokenized = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_eval_batch_size=32,\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\"\n",
        "\n",
        "#\n",
        "  )\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=dataset_tokenized[\"validation\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Qf7sQSH_XQ20"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}