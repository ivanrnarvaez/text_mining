{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "M5MhVlxlKxJF",
        "outputId": "b02cc34f-0bf0-4fdd-d444-cc6bd5210bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.3 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install contractions\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install fsspec==2023.9.2\n",
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5dffd1b"
      },
      "source": [
        "\n",
        "‚öôÔ∏è **Requerimientos importantes sobre el ejercicio**\n",
        "\n",
        "- El notebook debe ejecutarse **de principio a fin sin intervenci√≥n manual**.\n",
        "- Si utilizas librer√≠as que no est√°n incluidas por defecto en Google Colab, **aseg√∫rate de instalarlas dentro del notebook** (por ejemplo: `!pip install ...`).\n",
        "\n",
        "- Algunas celdas incluyen identificadores especiales que indican ciertas normas que **debes** respetar:\n",
        " - `#NO-MODIFY: DATA LOAD`  \n",
        "    üîí **No modifiques** el contenido de esta celda.\n",
        "\n",
        "  - `#NO-MODIFY: VARIABLE NAME`  \n",
        "    ‚úèÔ∏è Puedes modificar o a√±adir informaci√≥n **dentro de la celda**, pero **sin cambiar el nombre de la variable asignada**. No incluyas m√°s variables de las existentes en la celda.\n",
        "\n",
        "  - `#MODIFY: ADD INFO TO SOLVE FUNCTION`  \n",
        "    üîß Puedes modificar el **interior de la funci√≥n** para resolver la tarea, pero **no cambies su nombre, la cabecera ni el `return`**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w1C3ZBXHL0b"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WTXfHYTHQmo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5xkvK9cHSse"
      },
      "outputs": [],
      "source": [
        "# Add your imports here\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "import contractions\n",
        "from tqdm.autonotebook import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1KKTZUcH_xk"
      },
      "source": [
        "# üîç Ejercicio1: Detecci√≥n de profesiones en tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enunciado"
      ],
      "metadata": {
        "id": "gLDLR2bEHk2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejercicio vamos a trabajar con un conjunto de datos procedente de medios sociales online.\n",
        "\n",
        "Utilizaremos un subconjunto de los datos de la tarea 1 del shared task [**ProfNER**](https://temu.bsc.es/smm4h-spanish), centrada en la detecci√≥n de menciones a profesiones en tweets publicados durante la pandemia del COVID-19. El objetivo original de la tarea era analizar que profesiones podr√≠an haber sido especialmente vulnerables en el contexto de la crisis sanitaria.\n",
        "\n",
        "Para simplificar el ejercicio, he preparado una versi√≥n reducida del dataset original. Tu tarea ser√° entrenar un clasificador binario basado en la arquitectura Transformers, que, dado un tweet, determine si contiene una menci√≥n expl√≠cita a una profesi√≥n (etiqueta `1`) o no (etiqueta `0`).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6kx1_gwPQGcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ **Objetivos del ejercicio**\n",
        "\n",
        "A lo largo de este notebook, completar√°s las siguientes etapas para construir un clasificador de menciones a profesiones en tweets:\n",
        "\n",
        "1. **An√°lisis Exploratorio de Datos (EDA)**: Calcular estad√≠sticas b√°sicas del conjunto de datos (como el n√∫mero de ejemplos del training set, la distribuci√≥n de clases del dataset, la longitud media de los textos) o crear visualizaciones para cmprender mejor el contenido de los documentos usando wordclouds o histogramas.\n",
        "\n",
        "2. **Selecci√≥n y justificaci√≥n del modelo**: Elegir un modelo del Hub de Huggingface adecuado para los datos con los que se va a trabajar y el tipo de tarea a desarrollar.\n",
        "\n",
        "3. **Entrenamiento del clasificador**: Entrenar el modelo de forma reproducible y evaluar su rendimiento sobreel conjunto de datos de validaci√≥n, incluyendo un classification score y matriz de confusion\n",
        "\n",
        "4. **Generaci√≥n de predicciones sobre el conjunte de test**: Aplicar el modelo entrenado al conjunto de test, y guardar las predicciones en un archivo `.tsv` de 2 columnas `id` y `label` separadas por tabulador"
      ],
      "metadata": {
        "id": "3VHMEt0x-7UO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù **Criterios de Evaluaci√≥n**\n",
        "\n",
        "Tu trabajo ser√° evaluado seg√∫n los siguientes criterios:\n",
        "\n",
        "| Criterio                                            | Peso  |\n",
        "|-----------------------------------------------------|--------|\n",
        "| üîç An√°lisis exploratorio y preprocesamiento         | 20%   |\n",
        "| ü§ñ Selecci√≥n y justificaci√≥n del modelo             | 25%   |\n",
        "| üìÅ Formato y validez del archivo de predicciones    | 5%    |\n",
        "| ‚öôÔ∏è Ejecuci√≥n correcta del notebook (sin intervenci√≥n) | 10%   |\n",
        "| üìà Rendimiento del modelo sobre el conjunto de test | 30%   |\n",
        "| ‚úçÔ∏è Claridad y calidad de las explicaciones          | 10%   |\n",
        "\n",
        "\n",
        "\n",
        "üîî **Nota importante:**\n",
        "\n",
        "> El rendimiento del modelo se evaluar√° utilizando m√©tricas est√°ndar como el **F1-score** sobre el conjunto de test.\n",
        "\n",
        "> El archivo de predicciones debe respetar **estrictamente** el formato solicitado (`id` y `label`, separados por tabulador y con extensi√≥n `.tsv`).  \n",
        "  ‚ùó Si el archivo no cumple con este formato, **el ejercicio no podr√° ser evaluado en esa secci√≥n**.\n",
        "\n",
        "> El/la estudiante con el **mayor F1-score** obtendr√° la puntuaci√≥n m√°xima en el apartado de rendimiento. El resto de calificaciones se ajustar√°n de forma proporcional al mejor resultado\n"
      ],
      "metadata": {
        "id": "VPaXLRNeAElo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "‚öôÔ∏è **Requerimientos y reglas**\n",
        "\n",
        "- El notebook debe ejecutarse **de principio a fin sin intervenci√≥n manual**.\n",
        "- Si utilizas librer√≠as que no est√°n incluidas por defecto en Google Colab, **aseg√∫rate de instalarlas dentro del notebook** (por ejemplo: `!pip install ...`).\n",
        "\n",
        "- Algunas celdas incluyen identificadores especiales que indican ciertas normas que **debes** respetar:\n",
        " - `#NO-MODIFY: DATA LOAD`  \n",
        "    üîí **No modifiques** el contenido de esta celda.\n",
        "\n",
        "  - `#NO-MODIFY: VARIABLE NAME`  \n",
        "    ‚úèÔ∏è Puedes modificar o a√±adir informaci√≥n **dentro de la celda**, pero **sin cambiar el nombre de la variable asignada**. No incluyas m√°s variables de las existentes en la celda.\n",
        "\n",
        "  - `#MODIFY: ADD INFO TO SOLVE FUNCTION`  \n",
        "    üîß Puedes modificar el **interior de la funci√≥n** para resolver la tarea, pero **no cambies su nombre, la cabecera ni el `return`**.\n"
      ],
      "metadata": {
        "id": "3M19bykxA0ZP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj9IcJEwIB91"
      },
      "source": [
        "# Tu resoluci√≥n (rellena las celdas marcadas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOXm4JVwIElL"
      },
      "source": [
        "## Obtenci√≥n de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargamos los datos del [repositorio de Huggingface](https://huggingface.co/datasets/luisgasco/profner_classification_master)."
      ],
      "metadata": {
        "id": "gtOwX5HKCSfg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aiOIGd78IG3y"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: DATA LOAD\n",
        "from datasets import load_dataset, Dataset, DatasetDict, ClassLabel\n",
        "dataset = load_dataset(\"luisgasco/profner_classification_master\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El dataset contiene tres subsets:\n",
        "- **train** y **validation**: Contienen el identificador del tweet, el texto, y su etiqueta, que podr√° tener valor 1, si contiene una menci√≥n de una profesi√≥n; o valor 0, si no contiene una menci√≥n de una profesi√≥n.\n",
        "- **test**: El test set tambi√≠en contiene la informaci√≥n de label por un requerimiento de Huggingface, pero el contenido de esta variable es siempre \"-1\". Es decir que deber√©is predecir nuevas etiquetas una vez hay√°is entrenado el modelo utilizando el train y el validation set."
      ],
      "metadata": {
        "id": "nY-vjg88CfpW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "163nmdUnIG-P"
      },
      "source": [
        "## An√°lisis exploratorio de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para hacer el an√°lisis exploratorio de datos, transformamos cada subset a un pandas dataframe para mayor comodidad."
      ],
      "metadata": {
        "id": "umz-kP7yDDkL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkphOXpCIhqj"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: DATA LOAD\n",
        "dataset_train_df = dataset[\"train\"].to_pandas()\n",
        "dataset_val_df = dataset[\"validation\"].to_pandas()\n",
        "dataset_test_df = dataset[\"test\"].to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N√∫mero de documentos**\n",
        "\n",
        "Obten con la funci√≥n `get_num_docs_evaluation()` el n√∫mero de documentos del dataset de training y validation.\n",
        "\n",
        "> Recuerda incorporar la informaci√≥n para el c√°lculo dentro del a siguiente celda, sin modificar los atributos de entrada ni de salida de la funci√≥n, ni su nombre."
      ],
      "metadata": {
        "id": "kJ4lLzjODJTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_df.to_csv('train.csv', index=False)\n",
        "dataset_val_df.head()\n",
        "# dataset_test_df.head()\n",
        "#dataset_train_df[dataset_train_df['tweet_id'] == '1289869622187945984']"
      ],
      "metadata": {
        "id": "3VWuIgBGb_Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MODIFY: ADD INFO TO SOLVE FUNCTION\n",
        "def get_num_docs_evaluation(dataset_df):\n",
        "  # Modifica la funci√≥n.\n",
        "  num_docs = len(dataset_df)\n",
        "\n",
        "  # Si se quiere obtener √∫nicamente los que tienen textos distintos de vac√≠o\n",
        "  # num_docs = len(dataset_df[~dataset_df['text'].isna()])\n",
        "\n",
        "  # No modifiques el return\n",
        "  return num_docs\n"
      ],
      "metadata": {
        "id": "8v9AXqgMEh0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez generada la funci√≥n, puedes utilizarla posteriormente para calcular resultados y comentarlos"
      ],
      "metadata": {
        "id": "PVxQg5u_FPlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica la funci√≥n\n",
        "print(f\"El numero de documentos en TRAIN son: {get_num_docs_evaluation(dataset_train_df)}\")\n",
        "print(f\"El numero de documentos en VALIDATION son: {get_num_docs_evaluation(dataset_val_df)}\")\n"
      ],
      "metadata": {
        "id": "5l82IvmOFUBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N√∫mero de documentos duplicados**\n",
        "\n",
        "Obten con la funci√≥n `detect_duplicates_evaluation()` el n√∫mero de documentos duplicados del dataset de training y validation.\n",
        "\n",
        "> Recuerda incorporar la informaci√≥n para el c√°lculo dentro del a siguiente celda, sin modificar los atributos de entrada ni de salida de la funci√≥n, ni su nombre."
      ],
      "metadata": {
        "id": "TrxV3XkcFYhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_cviBv-IxHJ"
      },
      "outputs": [],
      "source": [
        "#MODIFY: ADD INFO TO SOLVE FUNCTION\n",
        "def detect_duplicates_evaluation(dataset_df):\n",
        "  # Modifica la funci√≥n.\n",
        "\n",
        "  num_duplicates = dataset_df.duplicated(subset='text').sum()\n",
        "  # No modifiques el return\n",
        "  return num_duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez generada la funci√≥n, puedes utilizarla posteriormente para calcular resultados y comentarlos"
      ],
      "metadata": {
        "id": "y_4zYMnOFjhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica la funci√≥n\n",
        "print(f\"El n√∫mero de documentos duplicados en TRAIN son: {detect_duplicates_evaluation(dataset_train_df)}\")\n",
        "print(f\"El n√∫mero de documentos duplicados en VALIDATION son: {detect_duplicates_evaluation(dataset_val_df)}\")"
      ],
      "metadata": {
        "id": "4AgUKaVQFjhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZq8ZXzkJk0t"
      },
      "source": [
        "**N√∫mero de documentos por cada clase:**\n",
        "\n",
        "\n",
        "Obten con la funci√≥n `analyse_num_labels_evaluation()` para calcular el n√∫mero de documentos de cada categor√≠a en el dataset\n",
        "\n",
        "> Recuerda incorporar la informaci√≥n para el c√°lculo dentro del a siguiente celda, sin modificar los atributos de entrada ni de salida de la funci√≥n, ni su nombre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvfd9yVVJo6-"
      },
      "outputs": [],
      "source": [
        "#MODIFY: ADD INFO TO SOLVE FUNCTION\n",
        "def analyse_num_labels_evaluation(dataset_df):\n",
        "  # Modifica la funci√≥n.\n",
        "  num_positives = len(dataset_df[dataset_df['label'] == 1])\n",
        "  num_negatives = len(dataset_df[dataset_df['label'] == 0])\n",
        "\n",
        "  # No modifiques el return\n",
        "  return num_positives, num_negatives"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez generada la funci√≥n, puedes utilizarla posteriormente para calcular resultados y comentarlos"
      ],
      "metadata": {
        "id": "zuRURsOqGVF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica la funci√≥n\n",
        "num_train_positives, num_train_negatives = analyse_num_labels_evaluation(dataset_train_df)\n",
        "num_val_positives, num_val_negatives = analyse_num_labels_evaluation(dataset_val_df)\n",
        "\n",
        "print(f\"El n√∫mero de documentos en TRAIN que son positivos son: {num_train_positives} y negativos son: {num_train_negatives}\")\n",
        "print(f\"El n√∫mero de documentos en VALIDATION que son positivos son: {num_val_positives} y negativos son: {num_val_negatives}\")\n"
      ],
      "metadata": {
        "id": "IUx_-_wFGVGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxXknuNvKK8f"
      },
      "source": [
        "**Distribuci√≥n de la longitud de los tweet en caracteres:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_rLDRgCKU8j"
      },
      "outputs": [],
      "source": [
        "dataset_train_df['char_len'] = dataset_train_df['text'].apply(lambda x: len(x))\n",
        "\n",
        "# Importamos las librer√≠as matplotlib y seaborn:\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig = plt.figure(figsize=(14,12))\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "sns.displot(data=dataset_train_df, x=\"char_len\", hue=\"label\", kde=True, legend=True)\n",
        "plt.legend([\"Incluye profesiones\", \"No incl. profesiones\"])\n",
        "# Definimos el t√≠tulo de los ejes:\n",
        "plt.xlabel('Caracteres-TRAIN', fontsize=16)\n",
        "plt.ylabel('Densidad', fontsize=16)\n",
        "\n",
        "# Finalmente mostramos el gr√°fico:\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_df[dataset_train_df['label'] == 1].describe()"
      ],
      "metadata": {
        "id": "UksGvhSGf4Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_df[dataset_train_df['label'] == 0].describe()"
      ],
      "metadata": {
        "id": "RpVVhqoXHJl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Aqu√≠ se observa que los tweets que contienen profesiones tienen a ser m√°s largos que los que no contienen. En el caso de los que si contientienen profesiones, el son en promedio m√°s grandes que los que no los contienen en un 22.36% de longuitud. El tweet mas largo es de 454 (Con profesi√≥n) y el mas corto es 10 caracteres(sin profesi√≥n). En general los textos donde se mencionan profesiones son mucho m√°s grandes que los que no los incluyen."
      ],
      "metadata": {
        "id": "71ZYnPHYGhjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_val_df['char_len'] = dataset_val_df['text'].apply(lambda x: len(x))\n",
        "\n",
        "# Importamos las librer√≠as matplotlib y seaborn:\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig = plt.figure(figsize=(14,12))\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "sns.displot(data=dataset_val_df, x=\"char_len\", hue=\"label\", kde=True, legend=True)\n",
        "plt.legend([\"Incluye profesiones\", \"No incl. profesiones\"])\n",
        "# Definimos el t√≠tulo de los ejes:\n",
        "plt.xlabel('Caracteres-VAL', fontsize=16)\n",
        "plt.ylabel('Densidad', fontsize=16)\n",
        "\n",
        "# Finalmente mostramos el gr√°fico:\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qFicqwFI51--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_val_df.describe()"
      ],
      "metadata": {
        "id": "P60RwVEv5_F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrtI16QKKNZq"
      },
      "source": [
        "**An√°lisis de contenido de los tweets**\n",
        "\n",
        "Para ello utiliza wordclouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZCi84lYK4jW"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "txt_cat0 = \",\".join(dataset_train_df[dataset_train_df['label'] == 0]['text'].to_list())\n",
        "txt_cat1 = \",\".join(dataset_train_df[dataset_train_df['label'] == 1]['text'].to_list())\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "es_stopwords = set(stopwords.words('spanish'))\n",
        "\n",
        "wordcloud = WordCloud(background_color='white',\n",
        "                      max_words=5000,\n",
        "                      contour_width=0,\n",
        "                      contour_color='plasma',\n",
        "                      stopwords=es_stopwords,\n",
        "                      normalize_plurals=True)\n",
        "\n",
        "wordcloud.generate(txt_cat0)\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud.generate(txt_cat1)\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "VXix1GpJ4_dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Se observar palabras clave como Coronavirus, Gobierno, Pandemia. Pero hay un gran presencia de textos a referencias. Despu√©s de la tokenizaci√≥n y limpieza se podr√≠a observar mejor lo que indica. Sin embargo es claro que en el caso donde no se meciona las profesiones, la idea clara es la presencia del virus y el confinaimiento. Mientras que en el caso de donde se menciona la profesion, la palabra sanitario, pandemia y gobierno est√° representadas con mayor tama√±o."
      ],
      "metadata": {
        "id": "oGebKDHFIZaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SOLO PARA TRAINING\n",
        "# Utilizando el procedimiento que vimos en clase para limpiar mejor los datasets\n",
        "# y observar mejor el contenido - Solo para analisis exploratorio\n",
        "\n",
        "# Eliminar espacios\n",
        "def eliminar_espacios(text):\n",
        "    return  \" \".join(text.split())\n",
        "\n",
        "# To lower\n",
        "def texto_to_lower(text):\n",
        "  return text.lower()\n",
        "\n",
        "def replace_contraction(text):\n",
        "  return contractions.fix(text, slang=True)\n",
        "\n",
        "# Tokenizador\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "def normalizar_tokens(documento):\n",
        "    # Dividir el documento en palabras\n",
        "    palabras =  tweet_tokenizer.tokenize(documento)\n",
        "\n",
        "    # Reemplazar URLs, menciones de Twitter y n√∫meros por los tokens correspondientes\n",
        "    for i in range(len(palabras)):\n",
        "        if palabras[i].startswith(\"http://\") or palabras[i].startswith(\"https://\") or palabras[i].startswith(\"www.\"):\n",
        "            palabras[i] = \"URL\"\n",
        "        elif palabras[i].startswith(\"@\"):\n",
        "            palabras[i] = \"MENTION\"\n",
        "        elif palabras[i].isdigit():\n",
        "            palabras[i] = \"NUM\"\n",
        "\n",
        "    # Unir las palabras de nuevo en un documento modificado\n",
        "    documento_modificado = ' '.join(palabras)\n",
        "\n",
        "    return documento_modificado\n",
        "\n",
        "# Lematizar\n",
        "import spacy\n",
        "nlp = spacy.load('es_core_news_md', disable=['parser', 'ner'])\n",
        "# es_core_news_md -> idioma espa√±ol\n",
        "\n",
        "def lematizar_eliminacion_tokens(texto):\n",
        "    # Procesar el texto con el objeto nlp\n",
        "    doc = nlp(texto)\n",
        "\n",
        "    # Lematizar el texto\n",
        "    lemas = [token.lemma_ for token in doc]\n",
        "\n",
        "    # Eliminar s√≠mbolos de puntuaci√≥n y stopwords\n",
        "    tokens_filtrados = [token for token in lemas if token.isalpha() and token.lower() not in es_stopwords]\n",
        "\n",
        "    # Unir los tokens filtrados en un nuevo texto\n",
        "    texto_procesado = ' '.join(tokens_filtrados)\n",
        "\n",
        "    return texto_procesado\n"
      ],
      "metadata": {
        "id": "v6CH_9QpW6IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_df['normalized'] = dataset_train_df['text'].progress_apply(lambda x: eliminar_espacios(x))\n",
        "dataset_train_df['normalized'] = dataset_train_df['normalized'].progress_apply(lambda x: texto_to_lower(x))\n",
        "dataset_train_df['normalized'] = dataset_train_df['normalized'].progress_apply(lambda x: replace_contraction(x))\n",
        "dataset_train_df['normalized'] = dataset_train_df['normalized'].progress_apply(lambda x: normalizar_tokens(x))\n",
        "dataset_train_df['normalized'] = dataset_train_df['normalized'].progress_apply(lambda x: lematizar_eliminacion_tokens(x))"
      ],
      "metadata": {
        "id": "mXKwwnyGWqAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_cat_norm0 = \",\".join(dataset_train_df[dataset_train_df['label'] == 0]['normalized'].to_list())\n",
        "txt_cat_norm1 = \",\".join(dataset_train_df[dataset_train_df['label'] == 1]['normalized'].to_list())\n",
        "\n",
        "# Para eliminar el ruido creadas en la depuraci√≥n\n",
        "es_stopwords.update([\"URL\", \"MENTION\", \"NUM\"])\n",
        "\n",
        "wordcloud.generate(txt_cat_norm0)\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "-qEEkmxS7pYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud.generate(txt_cat_norm1)\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "o7qV1zwl746s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FZUV6mSIJO1"
      },
      "source": [
        "## Tokenizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El texto del dataset no est√° preparado para ser introducido en un modelo Transformers. Lleva a cabo el proceso de tokenizaci√≥n."
      ],
      "metadata": {
        "id": "61YNAqIAG3aq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGA4Rz7UIDUz"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test_df.head()"
      ],
      "metadata": {
        "id": "IY_ID8ARVSRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecciona un modelo apropiado para la tarea:"
      ],
      "metadata": {
        "id": "kGZStD5MHBuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Recuerda que en la siguiente celda s√≥lo debes asignar un valor a model_name. No a√±adas m√°s informaci√≥n en la celda."
      ],
      "metadata": {
        "id": "famSsepTHJPx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svQiqzz_Lywz"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: VARIABLE NAME\n",
        "model_name = 'FacebookAI/xlm-roberta-base'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> El modelo seleccionado es el FacebookAI/xlm-roberta-base.\n",
        "Este modelo tiene una gran cantidad des descargas (13,501,854 en el √∫ltimo mes), es multilenguage (por lo que espa√±ol est√° incluido), es creado por Facebook (una compan√≠a de importancia en la industria), y tiene mucha informacion de soporte.\n",
        "Fuente:\n",
        "https://huggingface.co/FacebookAI/xlm-roberta-base"
      ],
      "metadata": {
        "id": "FTIJVHiajYtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puedes continuar con el proceso aqu√≠:"
      ],
      "metadata": {
        "id": "lMivggEnHQqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict, ClassLabel\n",
        "dataset_train_hf = Dataset.from_pandas(dataset_train_df).rename_column(\"label\", \"labels\")\n",
        "dataset_val_hf = Dataset.from_pandas(dataset_val_df).rename_column(\"label\", \"labels\")\n",
        "dataset_test_hf = Dataset.from_pandas(dataset_test_df).rename_column(\"label\", \"labels\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LdoDvBeEMTah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_hf = dataset_train_hf.cast_column(\"labels\", ClassLabel(num_classes=2)) # 0,1"
      ],
      "metadata": {
        "id": "zkJiNKPCUHk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "   AutoConfig,\n",
        "   AutoTokenizer,\n",
        "   AutoModelForSequenceClassification\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Tokenizar el texto\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "dataset_train_hf = dataset_train_hf.map(preprocess_function, batched=True)\n",
        "dataset_val_hf = dataset_val_hf.map(preprocess_function, batched=True)\n",
        "dataset_test_hf = dataset_test_hf.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "o7EvRUAJXjVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUel8a-FN0nB"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carga el model para ser ajustado posteriormente:"
      ],
      "metadata": {
        "id": "cOb2YTABXDv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "id2label = {0: \"NO_INCL_PROFESION\", 1: \"INCL_PROFESION\"}\n",
        "label2id = {\"NO_INCL_PROFESION\": 0, \"INCL_PROFESION\": 1}\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name,  num_labels=2, id2label=id2label, label2id=label2id)\n"
      ],
      "metadata": {
        "id": "IzfR_vm5XHOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeUuTSJpN9pN"
      },
      "source": [
        "### Configuracion training_args"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configura los par√°metros de entrenamiento del modelo.\n",
        "\n",
        "\n",
        ">"
      ],
      "metadata": {
        "id": "0Iy9mSLIIkju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Recuerda que en la siguiente celda s√≥lo debes asignar atributos a la variable training_args. No a√±adas  otras variables en la celda"
      ],
      "metadata": {
        "id": "ucWL18iUIqme"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mexUYxXoN9v7"
      },
      "outputs": [],
      "source": [
        "#NO-MODIFY: VARIABLE NAME\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"modelo_test\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.1,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        "    seed=52\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LosxnaBOIto"
      },
      "source": [
        "### M√©tricas de evaluaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define las m√©tricas de evaluaci√≥n"
      ],
      "metadata": {
        "id": "Xkg3xIBBIzdT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99-S1UCeOLAH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1_score = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy_value = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    f1_score_value = f1_score.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_value,\n",
        "        \"f1_score\": f1_score_value,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iVkFUvSMDjDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EmbLYGnFDjGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KAtsuG7mDjI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YFV7ugfOMtr"
      },
      "source": [
        "### Ajuste del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lleva a cabo el ajuste del modelo:"
      ],
      "metadata": {
        "id": "62fSC9hEI3ih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OYXvp-KOQ2J"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_train_hf,\n",
        "    eval_dataset=dataset_val_hf,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2) # Podriamoshacer une arly stop\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "m9c78TrnaygB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"output_model_save\")"
      ],
      "metadata": {
        "id": "-jeyzFc2a7l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qwerqwer"
      ],
      "metadata": {
        "id": "YxuuGxNUdtr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hi38oUIOevr"
      },
      "source": [
        "## Evaluacion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez llevada a cabo el entrenamiento, realiza la evaluaci√≥n del modelo."
      ],
      "metadata": {
        "id": "OPYzfaRRI-Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(dataset_test_hf)"
      ],
      "metadata": {
        "id": "rEOk5u3GXOhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9sPLjPWLMQNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Genera predicciones"
      ],
      "metadata": {
        "id": "Sf1-X0ozH1x1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genera predicciones sobre el test set. Recuerda que el archivo que generes y adjuntes al ejercicio debe tener dos columnas:\n",
        "\n",
        "\n",
        "| id         | label |\n",
        "|------------|-------|\n",
        "| 1234567890 | 1     |\n",
        "| 1234567891 | 0     |\n",
        "| 1234567892 | 0     |\n",
        "| 1234567893 | 1     |\n",
        "\n",
        "- El archivo debe estar en formato **TSV** (separado por tabuladores).\n",
        "- Debe contener exactamente **dos columnas**: `id` y `label`.\n",
        "- Es obligatorio incluir la **cabecera**.\n"
      ],
      "metadata": {
        "id": "4GyrGKTfJUIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "# Cambia esto por tu modelo\n",
        "hub_model_id = \"luisgasco/modelo_test\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(hub_model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(hub_model_id)\n",
        "\n",
        "dataset_tokenized = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_eval_batch_size=32,\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\"\n",
        "\n",
        "#\n",
        "  )\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=dataset_tokenized[\"validation\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Qf7sQSH_XQ20"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}